\subsection{Limitations of Perceptrons}

\mode<article>{
Connectionist neurons, or perceptrons, are limited in the variety of functions they are able to fit. 
When dealing with classification problems, a perceptron can only find a linear separation between any two classes. Irrespective of the neuron's non-linear activation function, the perceptron is regarded as a \emph{linear classifier}. Whether something falls on one side of the decision boundary or the other, is entirely based on applying a linear filter (i.e. $\vec w^{\top} \vec x$). The non-linearity $f(h)$ merely controls the value range of the neuron's response ($\{0,1\}, (-1,+1)$, \ldots) which helps in interpreting the neuron's response.\\

If observations for two different classes are distributed such that one cannot draw a line to separate them, then the two classes
are not linearly separable. In this case the perceptron will fail to find a suitable classification boundary between the two classes.
}

\begin{frame}\frametitle{Linear classifiers/linear decision boundaries}

Consider the following binary classification problems with $\vec x \in \R^2$.{}

\question{Can you find a line that separates the two classes for each case?}

\begin{figure}[h]
    \centering
	\includegraphics[width=0.8\textwidth]{img/and_or_xor_y}
	\mode<article>{
	\caption{(a) points are classified according to the AND function,
	(b) points are classified according to the OR function,
	(c) points are classified according to the XOR function.
	}
	}
	\label{fig:and_or_xor} 
\end{figure}

\mode<article>{
In \figureref{fig:and_or_xor}, particularly (a) and (b),
it is possible to draw a line that separates the classes. Therefore, the AND and OR functions are linearly separable.
A perceptron is capable of finding such a separating line. However, this does not apply to the third case, for the XOR function.
It is impossible to find a single line that will separate the classes.\\
The XOR function is not linearly separable.
}

\pause 

\question{Can we solve the XOR problem with multiple perceptrons? How?}\\

%\slidesonly{
%\begin{figure}[ht]
     %\centering
	%\includegraphics[trim=480 0 0 30, clip, width=0.4\textwidth]{img/and_or_xor_y.png}
	%\caption*{A single perceptron can not solve the XOR problem.}
	%\label{fig:xor} 
%\end{figure}
%}

\pause

\notesonly{
- Yes, think of it as a divide and conquer approach. We split the XOR problem into multiple sub-problems. 
A perceptron is used to solve each sub-problem.

If you're familiar with Boolean algebra, you might recognize the following expression for the XOR function:
}
\mode<presentation>{
\vspace{-10mm}
}

\begin{equation}
\label{eq:xor}
\mathrm{XOR}(x_1, x_2) = 
({\color{magenta}\,{x_1} \; \mathrm{AND} \; \overline{x}_2 \,})
 \;\; \mathrm{OR} \;\; 
({\color{green}\, \overline{x}_1 \; \mathrm{AND} \; x_2 \,})
\end{equation}

\end{frame}

\mode<article>{
For instance, the first perceptron ${\color{magenta}s^1_1}$ is tasked to separate the bottom-right cloud of points from the rest. 
A second perceptron ${\color{green}s^1_2}$ is used to separate the top-left cloud from the rest.
A third perceptron ${\color{blue}s^2_1}$ will then use the responses of both and respond to ``is \textbf{only one} of the two perceptrons \textbf{on}?''.

\figref{fig:build_xor} illustrates this approach. One need only recognize that each sub-problem is linearly separable.\\
}

\begin{frame}

\begin{figure}[ht]
    \centering
	\includegraphics[width=0.75\textwidth]{img/build_xor_crf}
	\caption{Solving sub-problems of the XOR problem.}
	\label{fig:build_xor} 
\end{figure}

\end{frame}

\begin{frame}

\begin{figure}[ht]
    \centering
	\includegraphics[width=0.75\textwidth]{img/build_xor}
	\caption{Solving sub-problems of the XOR problem.}
	\label{fig:build_xor} 
\end{figure}

\end{frame}


\mode<article>{
\figref{fig:xor_decisions} visualizes the response of the final neuron $s^2_1$ for different values of $x_1$ and $x_2$. The space is partitioned into one region in which the activity is $< 0.5$ and two disjoint regions (top-left \& bottom-right) in which the activity is $> 0.5$.
}

\begin{frame}
\begin{figure}[ht]
     \centering
     \savebox{\imagebox}{
     \mode<presentation>{
	 \includegraphics[width=0.4\textwidth]{img/xor_decision}
	 }
     \mode<article>{
	 \includegraphics[width=0.35\textwidth]{img/xor_decision}
	 }
	 }%
     \begin{subfigure}[t]{0.35\textwidth}
         \centering
         \usebox{\imagebox}% Place largest image
         \caption{}
         \label{fig:xor_decisions}
     \end{subfigure}
     \slidesonly{
     \hspace{4mm}
     \begin{subfigure}[t]{0.45\textwidth}
     }
    \notesonly{
     \hspace{1mm}
     \begin{subfigure}[t]{0.38\textwidth}
     }
         \centering
         \raisebox{\dimexpr.5\ht\imagebox-.5\height}{% Raise smaller image into place
         \includegraphics[width=0.9\textwidth]{img/xor_decision_s}
         }
         \caption{}
         \label{fig:xor_decisions_s}
     \end{subfigure}
	\caption{Identifying the decision boundaries.}
\end{figure}
\end{frame}

\newpage
\mode<article>{


What we are essentially describing is a Multilayered perceptron (MLP) with an architecture as illustrated in \figref{fig:xor_mlp_arch}. This MLP is made up of an output layer with a single output neuron ${\color{blue}s^2_1}$, and one hidden layer with two hidden neurons, ${\color{magenta}s^1_1}$ and ${\color{green}s^2_2}$ (the superscript denotes the layer index, the subscript denotes the neuron index within its layer). The terms ``neurons'' and ``nodes'' are treated as synonyms.
}
\begin{frame}

\begin{figure}[ht]
    \centering
	\includegraphics[width=0.4\textwidth]{img/xor_mlp_arch}
	\caption{simplified MLP architecture}
	\label{fig:xor_mlp_arch} 
\end{figure}



\end{frame}

\clearpage

\section{Feedforward MLP}
\subsection{Navigating through the indices}

\begin{frame}\frametitle{MLP}

\mode<article>{
The nodes in a feedforward neural network form a directed acyclic graph (DAG). There are no connections that feed back to a neuron in an earlier layer. A neuron can only be connected to another neuron that lies in a deeper layer in the network. We will eventually cover neural architectures that allow for feedback connections such as recurrent neural networks.
\figref{fig:mlp_arch} is an example MLP architecture (simplified by omitting bias nodes).
}

\begin{figure}[ht]
    \centering
	\includegraphics[width=0.7\textwidth]{img/section1_fig14}
	\caption{MLP architecture}
	\label{fig:mlp_arch} 
\end{figure}

\mode<article>{
Each node in the network is a connectionist neuron. This implies that every individual neuron extracts some feature from the ``input'' it receives.
The ``input'' to a neuron can come from the observed data $\vec x$ or it could be the response of another neuron from an earlier layer in the network.
This also implies that \emph{for every neuron} their exists a hyperplane which is represented by some weights $\vec w$ and a bias $\theta$.

$s^v_j$ denotes the \emph{activity} of the neuron $j$-th in layer $v$, while $s^{v'}_i$ denotes the \emph{activity} of the neuron $i$-th in layer $v'$.
$h^v_j$ and $h^{v'}_i$ denotes the \emph{total input} for neuron $(v,j)$ and neuron $(v',i)$, respectively.
The activity $s^{v'}_i$ is obtained by applying a transfer function $f^{v'}_i(\cdot)$ to the total input $h^{v'}_i$ of neuron $(v',i)$:
}

\begin{equation}
s^{v'}_i = f^{v'}_i(h^{v'}_i)
\end{equation}

\mode<article>{
where common choices for $f^v_j(\cdot)$, depending on the layer, can be:
\begin{itemize}
\item the identity function: $f^v_j(h) := h$. Often for the input layer, i.e. $s^0_j = f^0_j(h^0_j) = f^0_j(x_j) = x_j$
\item logistic sigmoidal or $\tanh(h)$. Common for hidden neurons and output neurons (for classification tasks).
\end{itemize}

$v$ is a literal that is used to denote a specific layer in the network.\\
$v=0$ describes the \emph{input layer} which holds the observations $\vec x$.\\
$v=L$ describes the \emph{output layer} of the MLP. All the layers in between, if any, are referred to as \emph{hidden layers}. For example, the scalar output of an MLP can be referred to using:


\begin{equation}
y(\vec x;\vec w) = f^L_1(h^L_1)
\end{equation}

$v'$, $v''$ are used to describe layers relative to the current layer $v$. $v'=v+1$ and $v''=v'+1=v+2$ is a very common but this depends on how exactly any two nodes are connected. Therefore, talking about layers $v$, $v'$ and $v''$ needs to be put into the context of the connections between neurons. To elaborate:

$w_{ij}^{v'v}$ measures the strength of the connection \underline{from} $(v,j)$ \underline{to} $(v',i)$. Seeing $v'v$ in the superscript of the weight tells us that the two neurons are directly connected (1 hop).

$w_{j0}^{v0}$ measures the bias $-\theta_{j0}^{v0}$ for neuron $(v,j)$. 

}

\end{frame}

\definecolor{darkgreen}{rgb}{0,0.6,0}
\begin{frame}
\only<1>{\frametitle{MLP}
}
\only<2>{\frametitle{MLP with \textbf{fully connected} layers}
}

\mode<presentation>{
\begin{figure}[ht]
     \centering
     \savebox{\imagebox}{
     \only<1>{
	 \includegraphics[width=0.4\textwidth]{img/section1_fig14}%
	 }
     \only<2->{
	 \includegraphics[width=0.4\textwidth]{img/section1_fig14_fc}%
	 }}%
     \begin{subfigure}[t]{0.5\textwidth}
         \centering
         \usebox{\imagebox}% Place largest image
     \end{subfigure}
     \hspace{10mm}
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \raisebox{\dimexpr.5\ht\imagebox-.5\height}{% Raise smaller image into place
         \includegraphics[width=0.9\textwidth]{img/section1_fig20_mini_vdi}
         }
     \end{subfigure}
\end{figure}
}

\mode<article>{

\begin{figure}
    \centering
	\includegraphics[width=0.33\textwidth]{img/section1_fig20_mini_vdi}
	\caption{Computing the total input for neuron $(v',i)$}
	\label{eq:compute_vdi}
\end{figure}
}

\mode<article>{
To formulate how neuron $(v', i)$ processes its input requires identifying the set of parent nodes $P(v', i)$ of neuron $(v',i)$. The weighted sum of the parent activations yields the total input ${\color{darkgreen}h^{v'}_i}$:
}
   
\only<-2>{
\begin{equation}
		{\color{darkgreen}h_i^{v'}} 
		:= 
		\kern-2ex
		\sum_{(\mu,k) \in P(v',\,i)}
		\kern-2ex
		w_{ik}^{v'\mu}\; 
		f_k^\mu\big( {\color{teal} h_k^\mu} \big)   
		\label{eq:total_input_vdi_pre},
\end{equation}
}

\mode<article>{
We now assume that an architecture with \emph{fully connected layers}. This implies that every node in layer $v$ is connected to all nodes in the subsequent layer $v'$ (except for the bias node in $v'$ because it has no parents). This type of connectivity allows us to simplify the notation as it implies that the set of parents $P(v',\,i)$ is essentially a vector of all activations in layer $v$:
}

\only<2>{
\slidesonly{
Full connectivity implies
}
\begin{equation}
P(v',i) := (
\underbrace{
s^v_0}_{\substack{=1\\ \text{(bias)}}
}
, s^v_1, \ldots, s^v_j, \ldots, s^v_{N_v})^\top,
\end{equation}

where $N_v$ is the number of hidden nodes in layer $v$. 

\notesonly{From this follows:}
}

\only<3>{

\begin{align}
		{\color{darkgreen}h_i^{v'}} 
		:=& 
		\kern-2ex
		\sum_{(\mu,k) \in P(v',\,i)}
		\kern-2ex
		w_{ik}^{v'\mu}\; 
		f_k^\mu\big( {\color{teal} h_k^\mu} \big) \\
		{\color{darkgreen}h_i^{v'}} \;
		\stackrel{\mathclap{
\substack{\text{\tiny fully}\\\text{\tiny conn.}}}
}{=}& 
		\kern1.5ex
		\sum_{j=0}^{N_v}
		w_{ij}^{v'v}\; 
		f_j^v\big( {\color{teal} h_j^v} \big)\\
		=& 
		\kern1.5ex
		{\vec w_{i}^{v'v}}^\top
		\kern-1ex
		\cdot  
		\vec f^v\big( {\color{teal} \vec h^v} \big)\\
		=& 
		\kern1.5ex
		{\vec w_{i}^{v'v}}^\top
		\kern-1ex
		\cdot
		\vec s^v\\
\end{align}

\slidesonly{\vspace{-5mm}}
here $\vec w_{i}^{v'v} := \big(
\underbrace{
w_{i0}^{v'v}}_{\text{bias}}
, w_{i1}^{v'v}, \ldots,w_{iN_v}^{v'v}\big)^\top \in \R^{N_v+1}$
}

\only<4>{
\notesonly{
Consequently, to compute} the total input for all $N_{v'}$ neurons in the layer $v$\notesonly{ we introduce the weight matrix 
$\vec W^{v'v} := \big({\vec w_1^{v'v}}^\top, {\vec w_2^{v'v}}^\top, \ldots, {\vec w_{N_{v'}}^{v'v}}^\top\big)$:}
\begin{equation}
\vec W^{v'v} = 
\left(
\begin{array}{cccccc}
\Big| & \Big| & & \Big| & & \Big| \\[3mm]
\vec w_{1}^{v'v} & \vec w_{2}^{v'v} & \cdots & \vec w_{i}^{v'v} & \cdots & \vec w_{N_{v'}}^{v'v}\\[2mm]
\Big| & \Big| & & \Big| & & \Big|
\end{array}
\right) \in \R^{(N_v+1) \times N_{v'}}
\end{equation}

Therefore,

\begin{equation}
		{\color{darkgreen} \vec h^{v'}} 
		=
		{\vec W^{v'v}}^\top
		\kern-1ex
		\cdot
		\vec f^v\big( {\color{teal} \vec h^v} \big)
		=
		\kern1.5ex
		{\vec W^{v'v}}^\top
		\kern-1ex
		\cdot
		\vec s^v
\end{equation}
}

\end{frame}
