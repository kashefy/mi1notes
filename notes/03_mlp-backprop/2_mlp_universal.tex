\section{MLPs are universal function approximators}

\begin{frame}\frametitle{\secname}

\mode<article>{
MLPs are universal function approximators. This means that, provided some assumptions are satisfied, they are capable of finding a function with which to map observations $\vec x \in \R^N$ to a their corresponding label $y_T$.
}

\begin{equation}
y: \vec x \mapsto y_T
\end{equation}

\mode<article>{
Moving forward we will focus on \emph{scalar} values for the label $y_T$ but also explain how things can be extended to labels of larger dimensions.
}

   \begin{block}{The universal approximation theorem by Funahashi (1989)}
	\small
    	Let $y_T{(\vec{x})}$ be a continuous, real valued function 
    	over a compact interval $K$ and     
		\begin{equation} 
		{y}{(\vec{x}; \vec w)} = \sum_{i=1}^M {w}_i^{21} 
		f\Big( \sum\limits_{j=1}^N {w}_{ij}^{10} 
		  \mathrm{x}_j - \theta_i \Big)
		 \end{equation}
    	be a three-layered MLP with a non-constant, bounded, 
    	monotonously increasing and continuous function 
    	$f: \mathbb{R} \rightarrow \mathbb{R}$.\\
		\vspace{4mm}
	   \pause

		Then there exists a set of parameters 
		$M, N \in \N$ and ${w}_i^{21}, 
		{w}_{ij}^{10}, \theta_i \in \R$ 
		such that for every $\varepsilon > 0$:
		\begin{equation}
			\max_{\vec{x} \in K} \Big| \, y_T{(\vec{x})} - {y}{(\vec{x}; \vec w)} \,\Big| 
			\leq \varepsilon
		 \end{equation}

  \end{block}
   \footnote
	{
	 Funahashi (1989) On the approximate realization of 
		continuous mappings by neural networks. Neur Netw, 2:183--192\\
		Hornik et al. (1989) Multilayer Feedforward Networks 
		are Universal Approximators. Neur. Netw, 2:359--366.
		}
  
\end{frame}

