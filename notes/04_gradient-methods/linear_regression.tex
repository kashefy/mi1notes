\section{Linear neuron for regression: analytical solution}

\subsection{Data, Model and Objective}

\begin{frame}\frametitle{Setting}

\underline{Data}:\\

\begin{table}[h]
\begin{tabular}{rl}
obserrvations & $x \in \R$ \\
label         & $y_{T} \in \R$ \\
Dataset       & 
$
\Big\{
	\left(x^{(1)}, y_T^{(1)} 
	\right)
	\,,\, \ldots \,,\,
	\left( x^{(\alpha)}, y_T^{(\alpha)} \right)
	\,,\, \ldots \,,\, 
	\left( x^{(p)}, y_T^{(p)} \right) 
\Big\}
$
\end{tabular}
\end{table}

\pause

\underline{Model}:\\

linear neuron:
\begin{equation}
    y(x; \vec w) = w_{0} + w_{1} x = \vec w^{\top} \vec x
\end{equation}

with $\vec x := (1, x)^{\top} \in \R^{2}$ and $\vec w := (w_{0}, w_{1})^{\top} \in \R^{2}$

\end{frame}

\begin{frame}

\underline{Cost function}:\\

Quadratic error

\begin{align}
E^{T} &= \frac{1}{p} \sum_{\alpha=1}^{p} \;
\frac{1}{2} \, \left( y(x^{(\alpha)}; \vec w)- y^{(\alpha)}_{T}\right)^{2}\\
&= \frac{1}{p} \sum_{\alpha=1}^{p}
\;e^{(\alpha)}
\end{align}

\mode<article>{
The term $\frac{1}{2}$ is merely for convinience when computing the gradient later.
}

\pause

\underline{Optimization}:\\

\begin{align}
E^{T} &\eqexcl \min_{\vec w}\\
\frac{\partial E^{T}}{\partial \vec w} &= \vec 0 \qquad\qquad \text{(satisfied by extrema)}
\label{eq:extremacond}
\end{align}
    
\end{frame}

\newpage

\subsection{Calculating the gradient}

%\mode<article>{
%Finding extrema that satisfy \eqref{eq:extremacond}:
%}

\begin{frame}\frametitle{\subsecname}
 
% temporarily change footnote marks to symbols so not to confuse with exponents
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}    

\begin{align}
\frac{\partial E^{T}}{\partial \vec w} 
\;\;&=\;\; 
\frac{1}{p} \sum_{\alpha=1}^{p} \; 
\frac{\partial e^{(\alpha)}}{\partial \vec w}\\
\;\;&\stackrel{\mathclap{
\substack{\text{chain}\\\text{rule}}}
}
{=}\;\;
\frac{1}{p} \sum_{\alpha=1}^{p} \;
{\color{blue}
\frac{\partial e^{(\alpha)}}{\partial y(x^{(\alpha)};\vec w)}
} 
\cdot
{\color{orange}
\frac{\partial y(x^{(\alpha)};\vec w)}{\partial \vec w}
}\\
\;\;&=\;\;
\frac{1}{p} \sum_{\alpha=1}^{p} \;
{\color{blue}
\big( y(x^{(\alpha)}; \vec w) - y_{T}^{(\alpha)} \big)
}
\cdot
{\color{orange}
\frac{\partial}{\partial \vec w} \vec w^{\top} \vec x^{(\alpha)}
}
%} \;\footnotemark \\
\intertext{using $\frac{\partial}{\partial \vec a}\left(\vec a^{\top} \vec b\right) = \vec b = (b_{1}, b_{2}, \ldots, b_{N})^{\top}$}
\;\;&=\;\;
\frac{1}{p} \sum_{\alpha=1}^{p} \;
{\color{blue}
\big( \vec w^{\top}\vec x^{(\alpha)} - y_{T}^{(\alpha)} \big)
}
\cdot
{\color{orange}
\vec x^{(\alpha)}
}\\
\;\;&=\;\;
\frac{1}{p} \sum_{\alpha=1}^{p} \;
\big( \vec x^{(\alpha)} \, \vec w^{\top}\vec x^{(\alpha)} - \vec x^{(\alpha)} y_{T}^{(\alpha)} \big)\\
%\;\;&=\;\;
%\frac{1}{p} \sum_{\alpha=1}^{p} \;
%{\color{orange}
%\vec x^{(\alpha)}
%}
%\cdot
%{\color{blue}
%\big( {\vec x^{(\alpha)}}^{\top}\vec w - y_{T}^{(\alpha)} \big)
%}\;\footnotemark \\
%\;\;&=\;\;
%\frac{1}{p} \sum_{\alpha=1}^{p} \;
%\Big( 
%{\vec x^{(\alpha)}}^{\top} \kern-.5ex \vec w\; 
%\vec x^{(\alpha)} 
%- y_{T}^{(\alpha)} \vec x^{(\alpha)}
%\Big)\\
\;\;&=:\;\; \vec g
\end{align}    

%\notesonly{ 
%\footnotetext{
%partial derivative calculated by using 
%$y(x^{(\alpha)};\vec w) = \vec w^{\top}\vec x^{(\alpha)}$ and 
%$\frac{\partial}{\partial \vec a}\left(\vec a^{\top} \vec b\right) = \vec b = (b_{1}, b_{2}, \ldots, b_{N})^{\top}$
%}
\footnotetext{
$\vec w^{\top}\vec x^{(\alpha)} = {\vec x^{(\alpha)}}^{\top} \vec w$
}
%}

% change footnote marks back to original scheme (numbers)
\renewcommand*{\thefootnote}{\arabic{footnote}}

\end{frame}

\subsection{Finding minima}

\begin{frame}\frametitle{\subsecname}

Constructing the Hessian matrix $\vec H$ to identify minima among the solutions that satisfy \eqref{eq:extremacond}:
\begin{equation}
\vec{H} \equiv \frac{\partial^2 E^T}{\partial\vec{w}^2} =
\left(\begin{array}{cccc}  
  \frac{\partial^2 E^T}{\partial w_0^2} &
  \frac{\partial^2 E^T}{\partial w_0\partial w_1} & \cdots &
  \frac{\partial^2 E^T}{\partial w_0\partial w_N} \\
  \frac{\partial^2 E^T}{\partial w_1\partial w_0} &
  \frac{\partial^2 E^T}{\partial w_1^2} & \cdots &
  \frac{\partial^2 E^T}{\partial w_1\partial w_N} \\
  \vdots & \vdots & \ddots & \vdots \\
  \frac{\partial^2 E^T}{\partial w_N\partial w_0} &
  \frac{\partial^2 E^T}{\partial w_N\partial w_1} & \cdots &
  \frac{\partial^2 E^T}{\partial w_N^2}
\end{array}\right)
\end{equation}

Let $\vec g$ denote the gradient vector: $\vec g = \frac{\partial E^{T}}{\partial \vec w}$. 
We recognize that the i-th \underline{row} in $\vec H$ resembles $\frac{\partial}{\partial w_i}\vec g^\top$.

Therefore,

\begin{align}
\vec H := \frac{\partial}{\partial \vec w}\vec g^\top
\;\;&=\;\;
\frac{1}{p} \sum_{\alpha=1}^{p} \;
\frac{\partial}{\partial \vec w}
\left(
\frac{e^{(\alpha)}}{y(\vec x^{(\alpha)}; \vec w)}
\cdot
\frac{\partial y(x^{(\alpha)};\vec w)}{\partial \vec w} 
\right)^{\kern-1ex\top}\\
\;\;&=\;\;
\frac{1}{p} \sum_{\alpha=1}^{p} \;
\frac{\partial}{\partial \vec w}
%\Big( 
%\vec x^{(\alpha)}
%\left({\vec x^{(\alpha)}}^{\top}\right) \vec w 
%\;-\; 
%\underbrace{
%\vec x^{(\alpha)}
%y_{T}^{(\alpha)} 
%}_{\text{indep. of }\vec w}
%\Big)\\
\Big( \vec x^{(\alpha)} \, \vec w^{\top}\vec x^{(\alpha)} - \vec x^{(\alpha)} y_{T}^{(\alpha)} \Big)^\top
\intertext{$
{\scriptstyle
\big(\vec x^{(\alpha)} (\vec w^{\top}\vec x^{(\alpha)})\big)^\top 
=~(\vec w^{\top}\vec x^{(\alpha)})^{\top}{\vec x^{(\alpha)}}^{\top}
=~{\vec x^{(\alpha)}}^{\top} \vec w~{\vec x^{(\alpha)}}^{\top}
}
$}
\;\;&=\;\;
\frac{1}{p} \sum_{\alpha=1}^{p} \;
\frac{\partial}{\partial \vec w}
~{\vec x^{(\alpha)}}^{\top} \vec w~{\vec x^{(\alpha)}}^{\top}
-
\frac{\partial}{\partial \vec w}
\underbrace{
\Big(\vec x^{(\alpha)} y_{T}^{(\alpha)} \Big)^\top
}_{\text{indep. of }\vec w}
%\Big( \vec x^{(\alpha)} \, \vec w^{\top}\vec x^{(\alpha)} - \vec x^{(\alpha)}^\top y_{T}^{(\alpha)} \Big)\\
\intertext{
using $\scriptstyle \frac{\partial~\vec a^{\top}\vec B~\vec c}{\partial \vec B} = ~\vec a~\vec b^{\top}$
}
\;\;&=\;\;
\frac{1}{p} \sum_{\alpha=1}^{p} \;
\frac{\partial}{\partial \vec w}
\Big( \vec x^{(\alpha)} \, \vec w^{\top}\vec x^{(\alpha)} - \vec x^{(\alpha)} y_{T}^{(\alpha)} \Big)^\top
\end{align}

\end{frame}
