@incollection{Vapnik1992,
title = {Principles of Risk Minimization for Learning Theory},
author = {Vapnik, V.},
booktitle = {Advances in Neural Information Processing Systems 4},
editor = {J. E. Moody and S. J. Hanson and R. P. Lippmann},
pages = {831--838},
year = {1992},
publisher = {Morgan-Kaufmann},
url = {http://papers.nips.cc/paper/506-principles-of-risk-minimization-for-learning-theory.pdf}
}
@book{haykin1994neural,
  title={Neural networks: a comprehensive foundation},
  author={Haykin, Simon},
  year={1994},
  publisher={Prentice Hall PTR}
}
@book{scholkopf2001learning,
  title={Learning with kernels: support vector machines, regularization, optimization, and beyond},
  author={Scholkopf, Bernhard and Smola, Alexander J},
  year={2001},
  publisher={MIT press}
}

@article{vapnik1999overview,
  title={An overview of statistical learning theory},
  author={Vapnik, Vladimir N},
  journal={IEEE transactions on neural networks},
  volume={10},
  number={5},
  pages={988--999},
  year={1999},
  publisher={Citeseer}}
}

@Inbook{Vapnik1998,
author="Vapnik, Vladimir",
editor="Suykens, Johan A. K.
and Vandewalle, Joos",
title="The Support Vector Method of Function Estimation",
bookTitle="Nonlinear Modeling: Advanced Black-Box Techniques",
year="1998",
publisher="Springer US",
address="Boston, MA",
pages="55--85",
abstract="This chapter describes the Support Vector technique for function estimation problems such as pattern recognition, regression estimation, and solving linear operator equations. It shows that for the Support Vector method both the quality of solution and the complexity of the solution does not depend directly on the dimensionality of an input space. Therefore, on the basis of this technique one can obtain a good estimate using a given number of high-dimensional data.",
isbn="978-1-4615-5703-6",
doi="10.1007/978-1-4615-5703-6_3",
url="https://doi.org/10.1007/978-1-4615-5703-6_3"
}
