\section{Ingredients for function fitting}

\mode<presentation>{
%\begin{frame}{Where are we?}
%\begin{columns}
    %\begin{column}{0.55\textwidth}
        %\tableofcontents[currentsection,hideallsubsections]
    %\end{column}
    %\begin{column}{0.45\textwidth}
        %\begin{center}
            %\includegraphics[width=0.8\textwidth]{img/meme_ingredients}
        %\end{center}
        %\begin{center}
        %Tuning the weights of a model to perform some task.
        %\end{center}
    %\end{column}
%\end{columns}
%\end{frame}
\begin{frame}
    \begin{center} \huge
        \secname
    \end{center}
    \begin{center}
        \includegraphics[width=0.4\textwidth]{img/meme_ingredients}
    \end{center}
    \begin{center}
        Tuning the weights of a model to perform some task.
    \end{center}
\end{frame}
}

\begin{frame}\frametitle{\secname}

    \mode<article>{
    Fitting a model to a desired function $y(\vec x)$ requires the following:
    }

    \begin{enumerate}
    \item<1-> 
    \mode<article>{A cost function  with the objective to optimize it, often a minimization problem:}
    \mode<presentation>{A cost function:\vspace{-10mm}}
	\begin{equation}
		e\tyxw \eqexcl \min_{\vec w}
	\end{equation}
    \item<2-> A performance measure, a criterion for \emph{model selection}.
    \mode<article>{Specifically, \\

    the generalization \textbf{error} $E^G$ which is defined as:}	
    \begin{equation} 
                \EGw \; := \; \left<\,e\,\right>_{y_T, \vec{x}; \vec w} 
                \; = \; \iint d \vec{x} \, dy_T \; 
                    P{(y_T, \vec{x})} \, e{\tyxw}
    \end{equation}
    \only<2>{
    Because $P{(y_T, \vec{x})}$ is not known, \mode<article>{we turn to the principle of empirical risk minimization (ERM).
    According to ERM we can approximate $\EGw$ by computing the} empirical average $\ETw$ using the available training data:
    $$
    \left\{\left(\vec x^{(\alpha)}, y_T^{(\alpha)}\right)\right\}, \alpha=1,\ldots,p
    $$.
    \mode<article>{The training error $\ETw$ becomes:}

    \mode<presentation>{\vspace{-10mm}}
    }
    \begin{equation}
    \only<2>{
    \text{batch training error:}\quad}\ETw=\frac{1}{p}\sum_{\alpha=1}^{p} \underbrace{e\tyxwalpha}_{e^{(\alpha)}}
    \end{equation}
    \mode<article>{
    where $e\tyxwalpha$ (or $e^{(\alpha)}$ for brevity) is the cost computed from the prediction for a specific observation $y(\vec x^{(\alpha)};\vec w)$ and its corresponding label $y_T^{(\alpha)}$. The superscript $^{(\alpha)}$ is used to index a specific point (sample) in the dataset.
    }
    \item<3-> A model with tunable parameters $\vec w$: connectionist neuron, MLP,\ldots
    \item<4-> A learning algorithm\mode<article>{ for finding the set of parameters in our model that will minimize the cost function.\\
    This can be done analytically (depending on some conditions) or through an iterative learning algorithm (e.g. gradient-based learning)}
    \end{enumerate}

\end{frame}

